IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022 3573

Factors Affecting On-Time Delivery
in Large-Scale Agile Software Development

Elvan Kula ,Member, IEEE, Eric Greuter, Arie van Deursen ,Member, IEEE, and Georgios Gousios

Abstract—Late delivery of software projects and cost overruns have been common problems in the software industry for decades.
Both problems are manifestations of deficiencies in effort estimation during project planning. With software projects being complex
socio-technical systems, a large pool of factors can affect effort estimation and on-time delivery. To identify the most relevant factors
and their interactions affecting schedule deviations in large-scale agile software development, we conducted a mixed-methods case
study at ING: two rounds of surveys revealed a multitude of organizational, people, process, project and technical factors which were
then quantified and statistically modeled using software repository data from 185 teams. We find that factors such as requirements
refinement, task dependencies, organizational alignment and organizational politics are perceived to have the greatest impact on on-
time delivery, whereas proxy measures such as project size, number of dependencies, historical delivery performance and team
familiarity can help explain a large degree of schedule deviations. We also discover hierarchical interactions among factors:
organizational factors are perceived to interact with people factors, which in turn impact technical factors. We compose our findings in
the form of a conceptual framework representing influential factors and their relationships to on-time delivery. Our results can help
practitioners identify and manage delay risks in agile settings, can inform the design of automated tools to predict schedule overruns
and can contribute towards the development of a relational theory of software project management.

Index Terms—Software engineering management, effort estimation, empirical studies, software companies

Ç

1 INTRODUCTION [8], but which factors have the most impact is not clear. We
lack an understanding of the relationships between these

LATE delivery and cost overruns have been common prob-
factors and how they impact on-time delivery.

lems in the software industry for decades. On average,
Effort estimation is also a major challenge in agile soft-

software projects run around 30 percent overtime [1]. This
ware development. Prior work [9] has found that around

percentage does not seem to have decreased since the
half of the agile projects run into effort overruns of 25 per-

1980s [2]. Even though effort estimation is at the heart of
cent or more. In agile settings, software is incrementally

almost all industries, it is especially challenging in the soft-
developed through short iterations to enable a fast response

ware industry. This is mainly due to the fact that software
to changing markets and customer demands. Agile projects

development is a complex undertaking, affected by a vari-
leverage short-term, iterative planning in which effort esti-

ety of social and technical factors. The overall perceived suc-
mates are progressively refined [10]. A particular challenge

cess of a software project depends heavily on meeting the
involves combining the flexible, short-term agile planning

time and cost estimates [3]. Improving effort estimation is
setting with the business needs for long term planning of

therefore a critical goal for software organizations: it can
availability of large pieces of functionality (often referred to

help companies reduce delays and improve customer satis-
as “epics” [11]). Most agile teams heavily rely on experts’

faction, while enabling them to efficiently allocate resources,
subjective assessment of team- and project-related factors to

reduce costs and optimize delivery [4], [5]. In spite of the
arrive at an estimate [12], [13]. However, these factors

availability of many estimation methods and guidelines [6],
remain largely unexplored [13]; further analysis is required

[7], on-time delivery in software development remains a
to investigate influential factors and how they impact delays

major challenge. Prior research identified a large number of
in agile projects.

factors that may influence the software development effort
By identifying and investigating influential factors, we

can obtain valuable insights on what data and techniques are

 needed to becomemore predictable at delivering software in
Elvan Kula is with the Delft University of Technology, 2628, CD, Delft,
Netherlands, and also with the ING Tech, 1102, MG, Amsterdam, The agile settings. An identification of themost influential factors
Netherlands. E-mail: E.Kula@tudelft.nl. can help software organizations increase the effectiveness

 Eric Greuter is with the ING Tech, 1102, MG, Amsterdam, The Nether- and efficiency of scheduling strategies by concentratingmea-
lands. E-mail: Eric.Greuter@ing.com.

 Arie van Deursen and Georgios Gousios are with the Delft University of surement and risk management activities directly on those
Technology, 2628 Delft, The Netherlands. E-mail: {Arie.vanDeursen, factors that have the greatest impact on on-time delivery.
G.Gousios}@tudelft.nl. Such knowledge can also guide future research on building

Manuscript received 14 Oct. 2020; revised 16 June 2021; accepted 13 July 2021. and evaluating software effort estimation techniques, meth-
Date of publication 2 Aug. 2021; date of current version 19 Sept. 2022. ods and tools. Furthermore, a deeper understanding of the
(Corresponding author: Elvan Kula.)
Recommended for acceptance by M. P. Robillard. interactions between influential factors can help in identify-
Digital Object Identifier no. 10.1109/TSE.2021.3101192 ing the root causes of delays, and developing tools and

This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/



3574 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

guidelines that can assist software organizations in improv-
ing their on-time delivery performance.

The goal of this paper is to identify the most relevant fac-
tors and their interactions that affect schedule deviations in
large-scale agile software development. To do so, we con-
duct a case study at ING, a large Dutch internationally oper-
ating bank with more than 15,000 developers. The teams at
ING develop software using an agile development process.
They work with epics to manage interdependent software
deliveries across multiple teams and iterations. We follow a
mixed-methods approach in which we combine expert-
with data-based strategies to derive, confirm and investi-
gate factors that impact the timelines of epic deliveries. We
conduct a survey with 635 software experts from ING, and
we analyze historic repository data from 185 teams and Fig. 1. Agile work breakdown structure.
2,208 epics to corroborate the survey findings. We extract
proxy measures from repository data that we map to the overruns in software projects. This work can be found in the
perceived influential factors and analyze their importance research areas of effort estimation and software project risk
in schedule deviations. Throughout our study, the follow- management. In this section, we provide background on
ing two research questions guide our work: large-scale agile software development, and we discuss

research on effort drivers and software project risks that
lead to schedule overruns. We also provide details on the

RQ1. Factor identification: Which factors are perceived to development context of the case company.
affect the timeliness of deliveries (RQ1.1), what is
their perceived level of impact (RQ1.2), and what are
the perceived types of interactions between these 2.1 Large-Scale Agile Software Development
factors and on-time delivery (RQ1.3)? A common way for agile companies to express user require-

ments is based on a five-level hierarchy introduced by Lef-
RQ2. Factor validation: How do the perceived influential fingwell [11] (shown in Fig. 1). Within strategic focus areas

factors impact schedule deviation in deliveries? called strategic themes, epics form high-level functional goals
Our survey results show that requirements refinement, for the product(s) [10]. Epics represent a large body of work

task dependencies, organizational alignment, organiza- that can be split into features, which in turn can be split into
tional politics and the geographic distribution of teams are user stories. Stories are short requirements or requests writ-
the factors that are perceived to have the greatest impact on ten from the perspective of an end user [14]. Finally, user
timely delivery. We find that factors interact hierarchically: stories are refined into development tasks, which denote the
organizational factors interact with people factors, which in technical work that needs to be done for a user story. Agile
turn impact the technical factors. The technical factors are teams work with a product backlog to keep track of the sta-
perceived to have a direct impact on the timeliness of soft- tus and business priority of work items [15].
ware delivery. Our data analysis reveals that the project Various agile planning frameworks for large projects
size, number of task dependencies, historical delivery per- (multi-team, multi-month) have been implemented success-
formance, team familiarity and developer experience are fully in large-scale software organizations [16], [17], [18].
the most important proxy measures that explain the sched- These frameworks rely on three levels of planning: release,
ule deviations in deliveries. iteration and daily planning [10]. The release plan (usually

By answering the research questions, we create a concep- 2-6 months) centers on epics [10], which often encompass
tual framework representing 25 factors and their interac- multiple teams and span over multiple iterations (e.g.,
tions that are perceived to affect the timely delivery of sprints in Scrum [19]). An iteration is a short, fixed-length
software at ING. The main contributions of this paper are: period (usually 1-4 weeks) in which a single development

team delivers a number of user stories.
 A set of factors and their interactions affecting the

timely delivery of software in large-scale agile devel-
opment. We order the factors by their relevance. 2.2 Factors Influencing Software Development

 A conceptual framework of on-time delivery that repre- Effort

sents influential factors and their interactions. This Prior work analyzed a variety of factors influencing the soft-

framework suggests multiple paths for action that ware development effort (so-called effort drivers). Trendowicz

may improve the timeliness of software deliveries. et al. [20] reviewed and divided the most commonly used
effort drivers into four categories: personnel factors (i.e., team
capabilities and experience [5], [9], [21], [22]), process factors

2 BACKGROUND (i.e., quality of methods, tools and technologies applied [23],
On-time delivery in software development remains a major [24], [25]), project factors (i.e., project resources and manage-
challenge and important topic of interest since improving it ment activities [26], [27]) and product factors (i.e., effort for
has a large economic benefit. Considerable research has requirements analysis, design and coding [28]). Personnel fac-
been directed at identifying factors that cause schedule tors and project factors are the topmentioned effort drivers in



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3575

agile projects [12]. As observed in previous studies [20], [29],
the accuracy of effort estimation methods depends on the
selection of relevant factors and the elimination of irrelevant
andmisleading factors.

Existing estimation methods can generally be classified
into expert-based and model-based approaches [5], [30].
Expert-based methods rely on human expertise to select rel-
evant factors, and are the most popular technique in both
agile and traditional (waterfall-like) projects [5], [31].
Model-based methods leverage data from past projects cov-
ering a certain initial set of factors in order to identify a sub- Fig. 2. Continuous delivery pipeline at ING.

set of factors that are relevant. Both methods have
significant practical limitations when applied individually. factors through a systematic literature review and classified
Experts decisions heavily rely on subjective assessment, them according to the SEI Taxonomy [52]. Related research
which may lead to inaccuracy and inconsistencies between [66], [67], [68] has also been done in using statistical analysis for
estimates. The effectiveness of data-based methods, on the evaluating risk factors in software projects.
other hand, largely depends on the quantity and quality of While various risk checklists and frameworks have been
available data. It is therefore likely that no single strategy proposed in earlier work, the relationships between risk fac-
will be the best performer for all settings [5]. Hence, recent tors and project delay remain largely unexplored. Our study
works [32], [33] propose to combine expert-based and data- complements prior work by providing further insights into
based methods – similar to this work. the relationships between factors and their implications for

Machine learning models have gained popularity as an on-time delivery performance.
alternative to model-based estimation methods. They have
achieved promising results in estimating effort for software 2.4 The Case Company
projects [34], [35], [36], [37], [38], and predicting the elapsed To address our research questions, we conducted a case
time required for bug-fixing or resolving an issue [39], [40], study at ING TECH, the largest IT department within ING
[41], [42], [43], [44]. Our study of factors affecting schedule that consists of 295 development teams distributed over
deviation in epics can provide further insights into impor- Europe, Asia and North America. TECH is responsible for
tant effort drivers that can contribute to the improvement of the development of ING’s main banking platforms and
estimation methods. advisory services that are being used by millions of custom-

ers worldwide. The department has significant variety in
terms of the products developed, the size and application

2.3 Software Project Risk Factors domain, as well as the programming languages used.
Risk factors are uncertain events that can pose a serious In recent years, ING has reinvented its organisational
threat to the successful completion of a software project [45]. structure, moving from traditional functional departments to
Several studies point out that ineffective risk management is a completely agile organisational structure based on Spotify’s
one of the main reasons for schedule overrun in software ‘Squads, Tribes and Chapters’ model [69]. The main purpose of
projects [46], [47], [48], [49], [50]. Risk management consists thismodel is to scale agile to hundreds of development teams.
of two main activities: risk assessment and risk control. Our Each development team consists of 5 to 9 members, and has a
current work focuses on risk assessment, which is the pro- Scrummaster and a product owner. The teams at ING TECH
cess of identifying risks, estimating the likelihood of their practice DevOps and follow Scrum as agile methodology.
occurrence and evaluating their potential effects [51]. In this They work in sprints of 1 to 4 weeks. To ensure that code can
work, we identify risk factors that lead to schedule overrun be deployed fast to production, ING put a fully automated
in software deliveries andwe evaluate their relative effects. software delivery pipeline in place. All development teams

Seminalwork in the area of risk assessment has been carried make use of the pipeline. The delivery pipeline contains sev-
out by Boehm [51] (‘Top 10 Software Risks”) and the Software eral specialized tools, as depicted in Fig. 2.
Engineering Institute [52] (“Taxonomy-Based Risk Identi- At ING TECH, epics are usually delivered in a time span
fication”). Examples of the most common risk factors are: of one to four quarters, i.e., three to 12 months. Epics should
unclear or changing requirements (e.g., [51], [53], [54], [55]), either be delivered as a whole within a quarterly cycle or, in
underestimated project complexity (e.g., [56], [57]), an unstable case of larger epics, in incremental software releases. The
organizational environment (e.g., [58], [59]), lack of manage- planning activities are led by tribe leads with active engage-
ment support (e.g., [57], [60]), bad commitment of the user ment of product owners tomanage inter-team dependencies.
([57], [61]) and personnel shortfalls (e.g., [51], [62]). Other stud-
ies focused on measurement of (the level of) risk [56], under-
standing the nature and types of risks [57] and mitigation of 3 RESEARCH METHOD

risk components [63]. Wallace et al. [64] reviewed literature in Our research method consists of an exploratory and confir-
the field and identified six dimensions of software project risks: matory phase. In the exploratory phase, we developed and
organizational environment, user, requirements, project com- distributed a survey to software experts to identify factors
plexity, planning & control and team risk. They analyzed the that are perceived to affect the on-time delivery of epics
relationships between risk dimensions and overall project per- (RQ1.1) and types of factor interactions (RQ1.3). In the confir-
formance. More recently, Menezes et al. [65] identified 148 risk matory phase, we applied data triangulation to corroborate



3576 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

the respondents’ perceptions and to extract more detailed provided definitions of factors to participants in the second
insights into the effects of influential factors.We conducted a survey. The first two authors have considerable experience
second survey with a different sample of software experts to working in the company, and they were able to explain the
order influential factors by their perceived level of impact factor definitions in vocabulary understood by the partici-
(RQ1.2), and we performed regression analysis using reposi- pants. Together with the last author, they also made sure
tory data to validate the impact of factors (RQ2). that the survey questions were coherent and consistent [70].

To avoid leading questions and biasing respondents, we

3.1 Collecting and Analyzing Survey Data phrased and ordered the questions in a sequential order of
activities [71]. The factors in the second survey were pre-

The main goal of the surveys was to gather the perceptions
sented in random order to the participants to reduce order-

of software experts at ING on factors affecting the timeliness
ing bias [74].

of epic deliveries and how much of an impact they have. To
design and execute our surveys, we followed methodologi-
cal guidelines from Kitchenham and Pfleeger [70], and 3.1.2 Survey Validation
Kasunic [71], for survey research in software engineering. After design, the survey instrument should be evaluated to

display areas for improvement [70], [71]. We piloted both

3.1.1 Survey Design surveys with 25 randomly selected employees from ING
TECH to refine the survey questions. The pilot versions

We developed two self-administered online surveys, which included an additional open-ended question at the end of
were composed of a mix of closed and open-ended ques- the survey asking respondents for feedback on the survey
tions. The first survey’s purpose was to identify influential contents. The respondents’ feedback allowed us to refine
factors and their interactions, and the second survey was used the survey questions. As part of the pilot run, we received 6
to assess the perceived level of impact of each of the identified fac- responses (24 percent response rate) for the first survey and
tors from the first survey. The surveys were organized into 5 responses (20 percent response rate) for the second survey.
two sections: a section aimed at gathering demographic
information and a section targeting the research questions.1

No reminder emails were sent. The pilot of the first survey
revealed that the wording of the survey question aimed at

We kept the number of survey questions to a minimum as RQ1.3 was unclear. The question asked respondents about
shorter questionnaires have been found to receive higher the types of relationships between factors, which respond-
response rates [70]. ents interpreted in different ways (e.g., sign of impact, cau-

The demographic sections of the surveys consisted of sality versus correlation, direct or indirect link). Since we
multiple-choice questions on the respondent’s role, overall wanted to collect descriptive information about factor inter-
experience in software development and experience within actions and do the classification of relationships ourselves,
ING. These demographic characteristics are important to we rephrased it as a more open-ended question in the final
assess the representativeness of participants [71] and they version of the survey. The initial version of the second sur-
have been shown to influence the reasons given for effort vey disclosed that the names of some factors (e.g., task
estimation errors in related work [73]. The research related dependencies versus technical dependencies) were ambigu-
section of the first survey contained open-ended questions ous to respondents. This prompted us to provide a list of
to gather unbounded and detailed responses on influential factor definitions in the final version of the second survey.
factors and types of factor interactions. For RQ1.1, we asked
experts which factors affect the timeliness of their teams’
epic deliveries. For RQ1.3, we included a follow-up open- 3.1.3 Survey Execution and Sampling Strategy

ended question asking how the reported factors influence Our target population was composed of the 2,850 employ-
the timeliness of epic deliveries. In the second survey, we ees that belong to the 295 development teams at ING TECH.
asked experts to rate the impact level of each identified fac- All these teams work with epic deliveries and are therefore
tor from the first survey (RQ1.2). We used four-point Likert relevant for our study. We received access to a mailing list
scale questions from “no impact” to “large impact” to get containing all team members, which became our sampling
specific responses. We also provided a separate “not frame. We were able to identify participants based on their
applicable” optional response in case a factor was not rele- email address and we had an overview of teams (team
vant to respondents. Here we also provided respondents names) they belong to. This enabled us to determine mem-
with a write-in question to probe for additional factors in bers’ participation and link survey responses to teams’
case any new ones might appear; we received 109 responses repository data for triangulation in RQ2.
to that question. We reviewed the responses manually and As recommended in survey guidelines [70], [71], [75], we
found that they were rephrasing one of the 25 factors or performed simple random sampling to obtain representa-
identifying a subcase of one of the factors. tive samples from our population. For our final surveys, we

We have taken multiple measures to make the survey excluded the 50 employees solicited in the earlier pilot sur-
questions understandable by the respondents [70]. We veys from our sampling frame. The final version of the first
included a brief paragraph on the survey’s start page featur- survey was distributed to 1,400 employees (one half of the
ing the purpose of the survey and an overview of the types population) in October 2019. These employees were sam-
of questions presented in each section. Furthermore, we pled uniformly at random across all teams at ING TECH.

We received 298 responses (representing 237 teams), corre-
1. The final survey instruments can be found in the supplemental sponding to a response rate of 21 percent. A majority (79

material [72]. percent) of teams had one respondent, 16 percent had two



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3577

respondents and remaining 5 percent had three respond- perceived types of relationships between influential factors
ents. The final version of the second survey was distributed and on-time delivery. We took the same approach as Jorgen-
to the other half of the population (another 1,400 employees) sen and Molokken-Ostvold [73]. We focused on direct, indi-
in November 2019. This second group did not include rect and contributory relationships to make a distinction
employees solicited in the first survey. We obtained 337 between simple, complex and condition-dependent types of
responses (representing 241 teams), corresponding to a interactions between factors. More discussions on types and
response rate of 24 percent. A majority (72 percent) of teams interpretations of reasoning models can be found in the
had one respondent, 18 percent had two respondents, 9 per- work of Pearl [79]. Possible interpretations of a factor X
cent had three respondents and the remaining 1 percent being a reason for schedule deviation are:
had four respondents.  There is a direct link between X and schedule deviation

As per our sampling plan for the surveys, the partici-
(i.e., X is a direct reason for deviation). We classified a

pants were invited using a personal invitation mail featur-
factor as having a direct relationship with on-time epic

ing the purpose of the survey and how its results can enable
delivery if it is explained to be an immediate reason

us to gain new knowledge of delay factors in epic deliveries.
for schedule deviation. For example, “unmanaged

Participants had a total of two weeks to participate in the
dependencies” is a reason that may immediately lead

surveys. To follow up on non-responders [70], we sent
to unplannedwaiting time and thus delay in an epic.

reminder emails to those who did not participate yet at the  There is an indirect relationship between X and sched-
beginning of the second week.

ule deviation (i.e., X leads to events that, in turn, lead
3.1.4 Survey Data Analysis to deviation). We classified a factor as having an indi-

rect relationshipwith on-time delivery if it is explained
The data we analyzed in this paper comes exclusively from

to affect schedule deviation through other factors or
the responses to the final surveys (i.e., the first survey

events. For example, “lack of organizational trust”
deployed in October 2019 and the second survey deployed

may lead to “errors during handoffs”, which in turn
in November 2019). The “not applicable” responses were

may result in “unmanaged dependencies”. “Lack of
omitted from the analysis set. For the analysis of RQ1.2, we

organizational trust” and “errors during handoffs”
used descriptive statistics to order factors by their perceived

are both indirect reasons of different distance to the
level of impact. For the analysis of RQ1.1 and RQ1.3, we

direct reason “unmanaged dependencies”.
performed inductive coding to summarize the results of the  The events leading to schedule deviation would have
open-ended questions. Coding samples are provided as

been harmlesswithout X (i.e., X is a contributory reason
examples in the supplemental material [72].

for schedule deviation). We classified a factor as hav-
Identifying Influential Factors. A common approach for

ing a contributory relationship with on-time delivery if
transforming qualitative data into quantitative data is cod-

it is described as a necessary condition for schedule
ing [76], [77]. For RQ1.1, we applied inductive coding (i.e.,

deviation rather than a direct or indirect reason.
inductive content analysis) during two integration rounds to

Assuming that “unmanaged dependencies” is a
derive influential factors from the open-ended survey

direct reason for schedule deviation, a contributory
responses. Each code in our coding scheme represents an

reason could be a “lack of a dependencymanagement
influential factor. We coded by statement and codes contin-

tool”. That is, delays caused by “unmanaged depend-
ued to emerge till the end of the process. In the first round,

encies” could have been prevented or reduced by
the first and the last author used an online spreadsheet to

effective dependencymanagement.
code a 10 percent sample (30 mutually exclusive responses)

For RQ1.3, we applied inductive coding during one inte-
each. They assigned at least one and up to four codes to

gration round to derive a combination of (intervening) fac-
each response. Next, the first and last author met in person

tors and their types of relationships to on-time epic
to integrate the obtained codes, meaning that similar codes

delivery. We classified each reported relationship as a
were combined or merged, and related ones were general-

‘direct’, ‘indirect’ or ‘contributory’ relationship using a sep-
ized or specialized if needed. When new codes emerged,
they were integrated in the set of codes. The first author arate code. Our interpretation of these relationships was

then applied the integrated set of codes to 90 percent of the based on the explanation of the respondent. For indirect

answers and the last author did this for the remaining 10 and contributory relationships, we also coded the interven-

percent of the responses. In the second round, the two ing factors that were mentioned. The first author performed

authors had another integrationmeetingwhich resulted into the coding and classification for all answers. The last author

the final set of codes. The final set contained three (13 per- did this for 20 percent of the answers. The resulting codes,
cent) more codes than the set resulting from the first integra- including the intervening factors that followed from indi-
tion round. We computed percent agreement and Cohen’s rect and contributory relationships, matched with codes
kappa [78] to assess inter-coder reliability on the final coding that were identified from open-ended responses to RQ1.1.
scheme. We measured substantial agreement between the No more new codes emerged in this process.
coders: percent agreement = 86 percent and k = 0.72. Then, To evaluate inter-coder reliability, the first and last author
together, the two authors grouped the factors into the five met in person to compare the types of relationships identified.
categories identified in thework of Chow et al. [3]. The result- There were a few borderline cases in which a reported rela-
ing 25 codes and five categories are summarized in Table 1. tionship would fit the indirect category as well as the contrib-

Classifying Types of Factor Relationships. For RQ1.3, we utory category. In such cases we tried to stay close to the
analyzed open-ended survey responses to investigate the formulation of the respondent. We classified a relationship as



3578 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

a contributory relationship only if an intervening factor was is being used by most of the development teams at ING
formulated as a necessary condition for the occurrence of TECH. The tool offers a wide range of metrics related to code
another factor (e.g., using an if-then statement). If it was not quality and unit tests execution. We extracted snapshots of
phrased as a conditional statement, then we marked the rela- SonarQube data of 190 teams that actively use the tool as part
tionship as an indirect one. Using Cohen’s kappa [78], we of their software delivery pipeline. Each snapshot is linked to
measured substantial agreement between the coders: k = 0.69 a delivery in ServiceNow based on team ID and time stamp of
and percent agreement = 83 percent. when the measurement was done in SonarQube. If the time

stamp of a SonarQube snapshot falls between the actual start
3.1.5 Survey Demographics date and actual end date of an epic in ServiceNow, the snap-
As mentioned earlier, the surveys contained a section aimed shot is considered to belong to the corresponding epic.
at gathering demographic information of the respondents, Although SonarQube offers a wide range of metrics, we only
namely, their role within ING, total work experience at ING, consider the subset ofmetrics that are collected by all teams at
total work experience in the software industry. A majority ING TECH. These metrics allow us to measure and compare
(66 percent) of the respondents self-identified as software the code quality of epics in our analysis set. For each snapshot,
engineer, while the rest identified themselves as manager or we extracted the metrics that teams at ING TECHmeasure to
team lead (19 percent), product owner (7 percent), software assess their coding performance:
architect (6 percent) or other (2 percent). The experience of  Coding standard violations: the number of times the
the respondents at ING ranged from one year (24 percent) to

source code violates a coding rule.
more than 20 years (12 percent) with a median of between  Cyclomatic code complexity: measured average cyclo-
one and five years (41 percent). The experience of the

matic complexity of all files contained in a snapshot.
respondents in the software industry ranged from one year  Branch coverage: the average coverage by tests of
(4 percent) to more than 20 years (24 percent) with a median

branches in all files contained in a snapshot.
of between 10 and 20 years (32 percent).  Failed test ratio: the number of failed tests divided by

the total number of tests executed during the devel-
3.2 Collecting and Analyzing Repository Data

opment phase of an epic.
To quantitatively assess the impact of the perceived influen-  Comment density: the percentage of comment lines in
tial factors presented in Table 1, we extracted proxy meas- the source code.
ures from multiple data sources at ING that capture the To account for differences in project size, we divided the
respondents’ intended meaning of the factors. In this sec- metrics Coding standard violations and Cyclomatic code com-
tion, we describe the datasets used and the linking process plexity by Source lines of code: the total number of lines of
that we applied to the datasets. The primary goal of our source code contained in a snapshot.
regression model is to explain, rather than predict; we want
to understand which proxy variables have a meaningful

3.2.3 Data Cleaning Process
relationship with schedule deviations in epics. Therefore,
we collected proxy variables that can be measured before To eliminate noise and missing values, we keep only the

and after an epic has been delivered. The mapping of proxy epics that meet the following conditions:

measures to perceived influential factors is shown in Table 3 1) The planned delivery date and actual delivery date have
and will be explained in Section 4.4. been set.

2) The epic has been assigned to a teamor group of teams.
3.2.1 Backlog Management Data 3) The description field has been set.
We extracted log data from ServiceNow, a backlog manage- 4) The epic has been completed (i.e., if its status is set to
ment tool used by a majority of teams at ING TECH.2 The completed).
dataset consists of 3,771 epics delivered by 273 teams at ING We also removed outliers that exceed two standard devi-
TECH between January 01, 2017 and December 31, 2019. The ations from the mean. For example, we removed six epics
dataset contains the following variables for epics: identifica- that had lasted longer than two years and that were, there-
tion number, creation date, planned start date, actual start date, fore, not representative for the rest of the dataset. The origi-
planned delivery date, actual delivery date and a textual descrip- nal dataset contained 3,771 epics. After linking and cleaning
tion. We acknowledge that the planned delivery date of a the data, the final dataset decreased to 2,208 epics from 185
delivery might change before actual development of the teams for which all data are present. This group of teams
delivery is started. Therefore, we consider only the planned overlaps with a majority (68 percent) of the teams that
delivery date as scheduled on the day that the development responded to the surveys.
phase is started. An overview of all variables and their
descriptions is provided in the supplemental material [72]. 3.2.4 Schedule Deviation Measures

There are a range of error measures used in effort estimation.
3.2.2 Code Quality Measurements Most of them are based on the Absolute Error (AE). Mean of
We extracted code quality metrics from SonarQube, a static Magnitude of Relative Error and Prediction at level l [80] have
code analysis tool in the software delivery pipeline at ING.3 It also been used in effort estimation. However, a number of

studies [81], [82], [83] have found that those measures bias

2. https://www.servicenow.com/ towards underestimation and are not stable when comparing
3. https://www.sonarqube.org/ effort estimationmodels. The Balanced Relative Error (BRE) [84]



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3579

has been recommended as an alternative estimation accuracy corresponding respondent’s identification number. The per-
measure. BRE is defined as ceived influential factors resulting from our manual coding

process are underlined. The proxy measures that we map to

If Act - Est  0; then BRE ¼ ActEst the perceived influential factors are dashed underlined.
Planned duration

4.1 (RQ1.1) Factor Identification
From the open-ended survey responses, we identified 25
factors that are perceived to affect the on-time delivery of

If Act - Est < 0; then BRE ¼ ActEst
;

Actual duration epic deliveries. A list of these factors is shown in the left-
hand column of Table 1. The factors are organized along the

where Act is the actual delivery date and Est is the five dimensions identified in the work of Chow et al. [3];
planned delivery date of an epic (as reported on the start organizational, process, project, people and technical.
date of the development phase). Act Est calculates the
difference in days between the actual delivery date and

4.1.1 Organizational Factors
planned delivery date: a positive difference corresponds to
underestimation (Act is later than Est), while a negative This category of factors concerns the uncertainty surround-

value corresponds to overestimation (Act is before Est). ing the organizational environment in which an epic deliv-

Actual duration is the time interval (in days) between the ery takes place. Many respondents report the importance of

actual delivery date and start date of the development organizational alignment for the on-time delivery of epics.

phase of an epic. Planned duration is the time interval (in A shared vision and mission are essential to ensure align-

days) between the planned delivery date and start date of ment between the implementation of an epic and its busi-

the development phase of an epic. We assess the relative ness strategy: “A clear management vision creates focus and
and absolute schedule deviation in epics using BRE and AE helps us align on business priorities and timelines across the
(measured in days), respectively. company.” [r216]

Another factor that is perceived to contribute to timely

3.2.5 Regression Analysis delivery is strong executive support. This includes the
active involvement of management in strategy execution

A common approach for measuring the impact of a number and the commitment of required resources. Respondent 285
of factors on estimation error is to use regression analysis. explains that: “It motivates us if management sufficiently partic-
For RQ2, we used regression analysis to quantitatively assess ipates in the preparation and performance review of delivery
the impact of combinations of perceived influential factors performance”.
on the schedule deviation in epics. We extracted 35 proxy In a related manner, respondents report delays related to
measures from backlog management data and code quality organizational politics. Bureaucratic structures in the orga-
measurements that can be mapped to the perceived influen- nization can hinder on-time delivery due to side steering:
tial factors. The proxy measures and their mapping to the “Management should trust teams to come up with realistic time-
influential factors are given in Table 3. An analysis of the lines instead of pushing deadlines. This will prevent last-minute
proxy data revealed that it does not meet the assumptions side steering and ad-hoc work.” [r39] Other factors in this cate-
for linear regression. Considering the need for an interpret- gory that are perceived to hamper the on-time delivery of
able model in our explanatory study, we decided to go for a epics are the geographic distribution of teams and a lack of
non-linear, spline-based regression modeling approach. We organizational stability (i.e., impact of organizational
applied MARS [85]; a multivariate, piecewise regression restructuring).
technique that can be used to model complex relationships
between a set of predictors and a dependent variable. We
used the proxy measures as predictors and the measured 4.1.2 People Factors
BRE as dependent variable. MARS divides the space of pre- People factors refer to qualities associated with a software
dictors into multiple knots, i.e., the points where the behav- development team that can affect the timeliness of deliver-
ior of the modeled function changes. This notion of knots ies. Factors that are perceived to contribute to the on-time
makes MARS particularly suitable for problems with high delivery of epics are team stability (i.e., low team member
input dimensions. The optimal MARS model is built using a turnover), strong skills and knowledge, team familiarity
backwards elimination feature selection routine that looks at (i.e., the amount of experience individuals have working
reductions in the generalized cross-validation (GCV) crite- with one another) and team commitment to on-time deliv-
rion as each predictor is added to the model. This procedure ery (i.e., motivation to deliver on-time). Teams that are
makes it possible to rank variables in terms of their contribu- more stable, skilled, familiar and committed to delivering
tion to the GCV.We used the default MARS setting provided epics on-time are perceived to deliver more often on-time.
by the EARTHpackage in R. Moreover, respondents point to the importance of effective

communication between teams, management and custom-
4 RESULTS ers when it comes to technical problems and project delays.

This section presents results on factors affecting delays in
epic deliveries, derived from survey responses and reposi- 4.1.3 Process Factors
tory data at ING. Example quotes from the survey are This category of factors refers to the effectiveness and matu-
marked with a [rX] notation, in which X refers to the rity of a software development team’s way of working. The



3580 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

TABLE 1 Some respondents indicate to feel more focused and effec-
Overview of 25 Factors That are Perceived to Affect the On- tive at work when they limit the amount of work in progress

Time Delivery of Epics at any given time.
Another prominent factor in this category is agile matu-

rity, which stands for the ability of a team to become more
agile over time. Respondents explain that they are able to
improve their agility, and thereby, on-time delivery, over
time through experience. Respondent 163 states that “It
helps to hold Scrum retrospectives and actually following up on
their outcome. This allows teams to come up with ways to avoid,
mitigate, or better handle impediments and other causes that
impact delivery”.

4.1.4 Technical Factors
The technical category represents factors related to the qual-
ity of the source code artifact, and the effectiveness of tech-
nology and tools used to produce that artifact. Technical
factors that are perceived to hamper timely delivery are
poor code documentation, lack of code quality, bugs or inci-
dents and insufficient testing. Well-defined coding stand-
ards are perceived to make teams more predictable in their
deliveries: “Higher quality standards will result in less incidents
and less time spent on code refactoring in the future. This will, in
turn, make our team more productive and predictable.” [r334]
Regarding testing, respondents mention to often fall behind
schedule when preparing and executing time/resource-
intensive types of tests, such as integration tests and perfor-
mance tests. Such tests can lead to delays due to delayed
availability of required infrastructure, unidentified depen-
dencies and late identification of defects.

Another factor that is perceived to delay epic deliveries is
a specific type of dependency — technical dependencies.
Technical dependencies can occur among different software
artifacts, e.g., source-code, architecture, hardware and tools.
Teams at ING work with project-specific repositories and
share codebases across teams within one application. As a
result, teams at ING are hampered by source code depen-

The factors are organized along five dimensions: organizational, people, pro- dencies across projects. Moreover, respondents perceive to
cess, technical and project. Percentage of respondents is the percentage of
survey respondents that mentioned the corresponding factor. The factors’ be delayed by the unavailability or instability of technology
Level of impact on delays was rated as 4 (large impact), 3 (moderate impact), and tools used for software development and software test-
2 (small impact) and 1 (no impact). Top 2 is the percentage of respondents ing (unreliable infrastructure).
that answered 4 or 3 for Impact level. WA is the weighted average of the Lik-
ert scale scores for Impact level. The factors’ Order numbers are based on the
ordering of the weighted averages. 4.1.5 Project Factors

This category represents the inherent complexity and uncer-
overall top mentioned factor is requirements refinement, tainty of a software project. Task dependencies constitute a
which refers to the process of defining epics and dividing top mentioned factor that is perceived to delay epic deliver-
them into user stories. Missing or lacking details in the ies. Task dependencies refer to dependencies among activi-
requirements is one of the main reasons for delay: “Most of ties in the workflow of collaborating software development
the time when we do not make the deadline, the team missed teams. Issues such as inconsistent schedules and unaligned
important information during refinement, which surfaced during priorities can cause delays for teams that collaborate in the
the sprint.” [r123] Here respondents also report the impor- same delivery chain: “Task dependencies occur when teams are
tance of frequent user involvement to manage user expecta- not end-to-end responsible for bringing software to production.
tions and avoid delays caused by scope creep. They create the need for a significant amount of hand-offs.” [r79]

Another prominent factor featured in this category is reg- Our respondents report several project characteristics
ular delivery. Respondents explain the importance of hav- that can affect on-time delivery: project size (i.e., the size of
ing a short cadence for on-time delivery: teams that software, the size of development teams and project dura-
regularly deliver production ready software are perceived tion), degree of project newness (i.e., the innovative nature
to be more predictable. Respondent 143 explains that of a project) and project security (i.e., whether the project
“Delivering software in shorter cycles enables our team to manage needs to go through resource-intensive security tests).
more complex projects and better predict our delivery capacity”. Regarding the latter, respondents explain that business-



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3581

TABLE 2
Types of Perceived Relationships Between Perceived Influential Factors and the Timeliness of Epic Deliveries (RQ1.3): Direct Rela-

tionship " , Indirect Relationship! , Contributory Link ? or Mentioned, but No Explicit Relationship

Factors can have three types of direct relationships with on-time delivery; they can lead to necessary rework (NR), unplannedwaiting time (WT) or deviations
in team effectiveness (TE). For indirect and contributory links, intervening factors are mentioned in the table. The percentages indicate the percentages of sur-
vey responses that mentioned the corresponding relationship.

and safety-critical applications are generally built and tested and regular delivery are the top 5 cited factors. They received
to much higher security standards, which may lead to unex- more than 86 percent responses in the large and moderate
pected delays in the quality assurance and security testing impact category. Based on the weighted average of impact
process. scores, requirements refinement (order #1), task dependencies

(order #2), organizational alignment (order #3), organizational
politics (order #4) and geographic distribution (order #5) are

4.2 (RQ1.2) Perceived Level of Impact the top 5 rated factors. They received a weighted average
For each factor, respondents were asked to rate the level of impact score of 3.38 or higher. The impacts of the top 15 rated
impact using Likert-type choices of “no impact”, “small factors are perceived to have large or moderate impact by
impact”, “moderate impact” and “large impact”. The right- over 76 percent of the respondents. Communication (order
hand column in Table 1 shows the perceived level of impact #25) has lowest perceived impact but 47 percent of the
of the factors as rated by the survey respondents. The Top 2 respondents still rated it to have large ormoderate impact.
percentage indicates the percentage of responses that rated Further analysis shows that respondents were quite
the factor as having “large impact” or “moderate impact”. consistent in their high ratings of most factors. The ratings
The Order represents the order of factors by the weighted for all factors have a standard deviation (SD) lower
average of their impact level scores. Close to 60 percent of the than 0.80, except for unreliable infrastructure (SD = 1.01),
respondents felt the factors are all moderately influencing project security (SD = 0.99), lack of code quality (SD = 0.97),
their on-time delivery. Task dependencies, requirements insufficient testing (SD = 0.96) and team commitment
refinement, organizational alignment, technical dependencies (SD = 0.96).



3582 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

4.3 (RQ1.3) Perceived Types of Factor the amount of work in progress, which in turn has a positive
Relationships impact on team effectiveness and timely delivery.

We investigated open-ended survey responses to identify The project factors are reported to have direct, indirect
direct, indirect and contributory relationships between 25 and contributory relationships with on-time epic delivery.
perceived influential factors (from Table 1) and on-time epic Unresolved task dependencies are perceived to immediately
delivery. As explained in Section 3.1.4, we focused on three result in delays through hand-offs and waiting times
types of relationships to distinguish between simple, com- between dependent teams. Organizational alignment and
plex and condition-dependent types of interactions between communication are perceived to contribute negatively to the
factors. An overview of the perceived types of relationships relationship between task dependencies and on-time epic
between factors and on-time epic delivery is shown in delivery. Respondents explain that task dependencies can
Table 2. Respondents mentioned three ways in which fac- only stay unresolved and lead to delays in environments
tors can have a direct impact on the timeliness of epics; fac- characterized by misaligned priorities and communication
tors can lead to unplanned waiting time (WT), necessary issues. Project size is perceived to be linked with factors
rework (NR) or changes in team effectiveness (TE). These are across all dimensions. Larger epics are associated with
orthogonal types of effects. WT and NR are described to more dependencies, parallel work, communication over-
lead to delays, while increased (perceived) TE (i.e., a team’s head, and increased testing and refinement effort. Innova-
capacity to achieve its goals and objectives) is reported to tive epics (project newness) are reported to involve time-
help with on-time delivery. Table 2 shows the relevant type intensive exploration activities and to be hampered by
(NR, WT or TE) for each perceived direct relationship. unforeseen obstacles, which can decrease team effectiveness

We found that the organizational factors, people factors and and lead to delays. A higher level of project security is per-
technical factors are perceived to interact hierarchically. The ceived to contribute positively to delays due to rework
organizational factors have an impact on the people factors, introduced by insufficient testing and lack of code quality.
which in turn affect the technical factors. From the organiza-
tional factors, executive support is perceived to have an Key findings from RQ1: We identified 25 factors that are
indirect impact on timely delivery through team stability perceived to affect the timeliness of epic deliveries.
and team commitment. Respondents believe that strong Requirements refinement, task dependencies, organiza-
executive support leads to more stable and highly moti- tional alignment, organizational politics and the geo-
vated teams. Geographic distribution is associated with graphic distribution of teams are perceived to have the
communication challenges and, thereby, reduced team greatest impact on on-time epic delivery. The factors
effectiveness and delay. From the people factors, team sta- interact hierarchically; the organizational factors interact
bility is observed to be positively related to skills and with people factors, which in turn impact the technical
knowledge and team commitment. Respondents explain factors. The technical factors are perceived to have a
that stable teams are more likely to develop competences direct impact on timely epic delivery.
over time and to take ownership of their work. Team com-
mitment and communication are perceived to increase team
effectiveness, and thereby, help with timely delivery. Skills

4.4 Studied Proxy Measures
and knowledge is reported to have an indirect impact
through bugs or incidents and lack of code quality. We formulate 35 proxy measures to map to the perceived

Respondents point out that teams with more experienced influential factors. We extract the proxy measures from the

members are faster at finding faults in software, and resolv- datasets described in Section 3.2 to fit a regression model to

ing unforeseen bugs and incidents. From the technical cate- quantitatively assess the impact of the perceived influential

gory, all factors are perceived to have a direct impact on on- factors. Table 3 provides definitions for the proxy measures

time epic delivery. Respondents explain that problems and their mapping to the perceived factors. Some of the

related to technical dependencies, lack of code quality, bugs proxy measures are self-descriptive and for others we

or incidents and insufficient testing can introduce necessary explain the measure below. The proxies of the technical fac-

rework. Technical dependencies are also perceived to have tors are described in Section 3.2.2. We take the median

an indirect impact on on-time delivery through lack of code across teams or individual team members to produce meas-

quality. Respondents indicate that technical dependencies ures that are representative of the group as a whole. For

can introduce dependency problems, resulting in more individual data, such as developer age, we take the median

complex and less maintainable source code. of team members working on an epic. For team data, such

In general, the process factors are not perceived to have as team stability, we take the median of teams working on

relationships with factors from other categories. One excep- an epic.

tion is the link between requirements refinement and task We were not able to measure proxies for organizational

dependencies: respondents believe that an effective refine- alignment, organizational politics, executive support, (qual-

ment process should reveal task dependencies, thereby ity of) communication and technical dependencies. ING is

enabling teams to minimize the likelihood of delays caused not collecting quantitative data on these factors. It was also

by dependencies. Moreover, the most often reported rela- not possible to collect such data ex post facto.

tionship is the direct impact of requirements refinement on
on-time epic delivery. Respondents report that refinement 4.4.1 Motivation for Mapping
can prevent rework that is caused by unclear requirements. Organizational Factors. To quantify the global distance
Regular delivery is perceived to lead to more consistency in between teams, we calculate global-distance based on the



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3583

TABLE 3
Mapping of Proxy Measures to the Perceived Influential Factors (From Table 1)

The Description column provides a description of the proxy measure.

Global Distance Metric proposed in related work [86]. We comments and questions that could result in more updates
calculate the metric for pair-wise combinations of teams and and potentially a delay. To assess agile maturity, we calcu-
take the maximum value. To assess organizational stability, late velocity and team-point ; both have been shown to be
we calculate nr-changed-leads ; this has been shown to influ- representative of maturity in related work [10], [87]. We
ence the success of software projects in earlier work [57]. extract acc-criteria to measure user involvement in the defi-

Process Factors. To assess the refinement quality of an nition of an epic. The backlog management data contains a
epic, we calculate state-ready to determine whether the special ‘acceptance criteria’ field that indicates whether
‘status’ field of an epic was set to ‘refinement ready’ before teams consulted their customer(s) to define acceptance
the start of its planning phase. The intuition here is that criteria.
epics that are not clearly defined will miss important infor- Project Factors. We quantify task dependencies as the out-
mation that is needed for planning and that could poten- going degree (out-degree ) of dependencies of an epic; this
tially result in delay. We also calculate nr-updates ; the has been shown to predict delay in related work [67]. To
intuition here is that problematic epics will raise more assess project newness, we determine the novelty of an



3584 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

epic based on a special ‘business goal’ field in backlog man- TABLE 4
agement data. This field indicates whether an epic contrib- The MARS Model for the BRE Values of Epic Deliveries (RQ2)

utes to a ‘business transformation’, ‘business continuity’ or
‘compliance-related obligation’. Epics that contribute to a
business transformation are marked as novel. To measure
project security, we determine security-level , which indi-
cates whether an epics needs to go through a mandatory,
resource-intensive security testing procedure.

People Factors. To calculate the number of senior develop-
ers working on an epic (dev seniority), we retrieve the
expertise levels of developers as specified in backlog man-
agement data. At ING, the five-stage Dreyfus model [88] is
employed to evaluate the expertise of developers based on
their experience in the software industry. We marked devel-
opers with a Dreyfus skill level of 3 (‘competent’) and
higher as senior. To assess a team’s commitment to on-time
delivery, we calculate hist-performance; team commitment
has been shown to be positively related to team perfor-
mance and estimation accuracy in related work [64], [89],
[90], [91].

4.5 (RQ2) Factor Validation
How do perceived influential factors impact schedule deviation in
epic deliveries?

To answer this research question, we applied MARS to
quantitatively assess the impact of combinations of proxy
measures on the BRE values in epic deliveries. Table 4
presents the optimal MARS model for BRE values based on
all proxy measures presented in Table 3. The columns
show, from left to right, the beta factor coefficients bm
denoted as BFm, the basis functions selected as significant
covariates in the model, the coefficient values estimated and
a visualization of the relationship between the independent
variable and BRE value. The value of the beta factor implies
the magnitude of effect of the basis function (i.e., variable Adjusted R2: 0.672. The variables in the models are ordered by importance (see

Fig. 3).
effect) on the BRE value. For the effect of each basis func-
tion, maxð0; x tÞ is equal to ðx tÞ when x is greater than
t (the knot value); otherwise the basis function is equal to be explained as follows. The delays in epics tend to increase
zero. with a higher number of sprints. If the number of sprints in

As shown in Table 4, the MARS model contains 20 basis an epic is lower than 8, the schedule deviation will increase
functions and 13 proxy measures. The selected proxy meas- by 0.0142 per sprint (indicated by BF1). If the number of
ures represent all five of the factor dimensions. They are sprints exceeds 8, then the schedule deviation value will
effective in explaining 67 percent of the variation in the BRE increase faster by 0.0339 per sprint (indicated by BF2). Other
values of epic deliveries (adjusted R2: 0.672.). Fig. 3 pro- proxy measures that contribute to delay in epics are
vides a ranking of the proxy measures by order of impor- out-degree, team-size, nr-unplanned-stories and nr-stories.
tance. Proxies that have no impact on BRE are not shown. They have a two-sided, positive relationship with BRE with
The importance is calculated as the relative importance of a single knot. This indicates that delays in epics tend to
proxies in terms of reductions in the GCV estimate increase with larger teams and with more outgoing depen-
of the prediction error as each proxy measure is dencies, unplanned stories and planned stories. The proxies
included. From this figure, we observe that nr-sprints, security-level, nr-changed-leads, nr-incidents and
out-degree, hist-performance, dev-age-ing, team-existence dev-workload-points can also contribute to delays: they
and team-size have the greatest impact on schedule devia- have a right-sided, positive relationship with BRE with a
tion in epics. Their importance values range from 15 to 21 single knot. This means that when these proxies exceed
percent. their corresponding knot value, the delay in epics tends to

Factors associated with delay show a rising relationship in increase. For example, the beta factor BF15 shows the non-
the rightmost column of Table 4. From Table 4, beta factors linear effect of nr-changed-leads on BRE which can be
BF1 and BF2 account for the nonlinear delay effect of described as follows. If the teams’ tribe lead changed less
nr-sprints, the most important variable in the MARS model. than two times during the current and previous epics, it has
The number of sprints in an epic is positively related to the a negligible effect on the BRE value (indicated by BF0).
BRE values with a knot at t ¼ 8. The effect of nr-sprints can However, if the number of changed tribe leads exceeds two,



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3585

5 A CONCEPTUAL FRAMEWORK OF ON-TIME
DELIVERY

We organized the findings from our survey and regression
analysis into a conceptual framework, presented in Fig. 4.
The framework captures the 25 perceived influential factors
(from Table 1) and how they relate to on-time delivery. The
connections between factors are derived from the reported
types of relationships in Table 2. The directions are derived
from the descriptions of factors in Section 4.1 and the rela-
tionships in Section 4.3.

Practitioners can use our conceptual framework to iden-
tify and manage the risks associated with on-time software
delivery. For example, in our regression model, historical
delivery performance is one of the most important proxy
measures that affect schedule deviations in epics. Epics
assigned to teams having a high percentage of delayed epics
are at a high risk of being delayed. We recommend project
managers to identify the teams that are involved with many
delayed epics in the past, and consider allocating more time
for their work or training them to successfully estimate

Fig. 3. Importance of proxy measures in MARS model for BRE. effort and deliver on-time.
Our study also provides practitioners with a comprehen-

then the BRE value will increase by 0.0344 for every addi- sive list of factors and proxy measures that should be col-

tional change in tribe lead. The effects of security-level are lected and analyzed to derive useful models that can be

similar. If the security level is higher than 0.85, then the BRE applied to improve on-time delivery. An assessment of the

increases by 0.1288. Given that (security-level is a binary most significant factors would be a good starting point for

variable, this means that in practice the BRE values of epics further exploring influential factors in other settings. By

that need to pass the mandatory security testing procedure weighing and analyzing the qualitative and quantitative

are 0.1288 higher than the BRE values of epics that do not attributes of factors, practitioners can choose the most

need to go through this procedure. important factors that influence their on-time delivery.

Factors associated with on-time delivery show a downward Moreover, our analysis shows that expert- and data-based

relationship in the rightmost column of Table 4. As indi- selection methods identified different (only partially over-

cated by BF5-9 and BF16, the proxies (hist-performance, lapping) sets of relevant factors. Therefore, we recommend

dev-age-ing, team-existence and stability-ratio help with software practitioners to combine selection methods to

the timely delivery of epics. The delay in epics tends to extract more detailed insights and gain a better understand-

decrease for teams that were less often involved with delay ing of their software development processes.

epics in the past and teams that have existed longer in their The design of an effective strategy to improve on-time

current composition. The delay can also decrease with a delivery must recognize the relationships between influen-

higher developer experience at ING and higher team stabil- tial factors. The relationships in our conceptual framework

ity up to the corresponding knot values. are operationalized by reported relationships. We can there-

Absolute Deviation. Further analysis of the absolute devia- fore not reason about causal links between factors. How-

tions in epics showed that an overlapping set of 10 variables ever, our framework does enable us to form hypotheses that

is effective in explaining 61 percent of the variation in the AE could lead to actionable insights and may suggest corrective

values. The proxymeasure nr-teams emerged as statistically actions to address the root causes of delay in similar

significant, while nr-unplanned-stories, nr-changed-leads, development contexts. Our results suggest that addressing

nr-incidents and dev-workload-points were not selected in factors that have a direct relationship with on-time delivery

the model for AE values. Moreover, the proxies have slightly should directly lead to an improvement in on-time delivery.

different importance values:out-degree (15), nr-sprints (14), For example, our survey respondents believe that a lack of

(team-existence (13), dev-age-ing (12), hist-performance (11) code quality leads to necessary rework, and thereby, delay

and (nr-stories (10) have the greatest explanatory power for in epics. We therefore hypothesize that code quality

the absolute deviation. improvements may reduce the likelihood of rework and
improve the on-time delivery performance. Moreover, our

Key findings from RQ2: A set of 13 proxy measures is results suggest that improvements in indirectly influential
effective in explaining 67 percent of the variation in the factors lead to improvements in intervening factors, which,
BRE values of epics. The project size, number of task in turn, improve the timeliness of deliveries. For example,
dependencies, historical delivery performance, team executive support is perceived to have a positive impact on
familiarity and developer experience at ING have the team stability and team commitment, which in turn leads to
greatest explanatory power for schedule deviations in improved team skills, high-quality code and less bugs or
epics. incidents. We hypothesize that establishing stronger execu-

tive support may lead to more stable, committed and highly



3586 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

Fig. 4. A conceptual framework of on-time delivery (based on survey responses and quantitative data from the case company). The dimensions/cate-
gories of factors are shown in blue rectangles. The 25 perceived influential factors are visualized as ellipses connected by directional arrows indicat-
ing their perceived relationships. For readability purposes, the arrows starting from project size are partially omitted. For each relationship, the
direction (positive + or negative -) is shown. Factors are found to have three types of direct relationships with the timeliness of deliveries; they can
lead to necessary rework (NR), unplanned waiting time (WT) or changes in team effectiveness (TE). The star symbols indicate factors that are rated
among the top 10 most relevant factors in survey data or repository data.

skilled teams that are better able to maintain code quality Our results provide new insights into the relative effects of
and resolve delays caused by bugs and incidents. The paths these factors. Our respondents perceive requirements
for action that can be inferred from our conceptual frame- refinement, task dependencies and organizational align-
work indicate that on-time software delivery requires atten- ment to have the greatest impact on the timing of their
tion across many factors, and that both social and technical deliveries. While requirements-related issues are top-cited
factors may need to be addressed to enable continuous risk factors in literature [65], task dependencies and organi-
improvement. zational alignment have not received much attention. They

have only been investigated in the context of scaling agile
methods [93]. Further research is required to investigate the

6 DISCUSSION importance of these factors in the context of on-time
New Influential Factors. Our study has identified additional delivery.
factors that influence on-time delivery and which, to the Prior work [12] has shown that team- and project-related
best of our knowledge, have not been covered in the current factors are the most often mentioned effort drivers by agile
literature. We found that team familiarity is associated with practitioners. However, the relative importance of these fac-
timely software delivery. While prior research [92] has tors has not been investigated before. The regression analy-
shown that familiarity is beneficial to team performance, it sis we presented as part of RQ2 confirmed that the proxy
has not been investigated in the context of on-time delivery measures of project factors (i.e., task dependencies, project
before. Our respondents believe that familiarity between size) and team factors (i.e., team commitment, team famil-
team members improves the team coordination and helps iarity and skills & knowledge) have the greatest impact on
in adapting to environmental changes. This indicates that schedule deviations. The project factors are found to have a
for a better on-time delivery performance project managers slightly greater impact than task factors. We did not find a
should not only focus on keeping teams stable, they should significant relationship between the code quality measure-
also track and support teams to build familiarity over the ments and schedule deviations in epics. Poor code quality
long term. Moreover, agile maturity emerged as a new fac- and documentation have also emerged as perceived influen-
tor that is perceived to affect on-time delivery. The survey tial factors in other studies [94], [95]. More in-depth studies
respondents rated this factor among the top 10 most influen- are needed to corroborate the perceived impact of source
tial factors. The survey responses point out that a growing code quality on on-time delivery.
agile maturity enables teams and ultimately the organiza- Role of Organizational Environment. Our respondents
tion to continuously improve their on-time delivery reported the importance of several organizational factors for
performance. on-time delivery. The factors organizational alignment, orga-

Relevance of Factors. The 25 influential factors we pre- nizational politics and geographic distribution were rated
sented as part of RQ1 relate to previous research in effort among the top 5 most influential factors in our survey.
estimation [20] and software project risk management [65]. Among these, organizational politics and organizational



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3587

alignment have been shown to impact project performance are trying to optimize. In reality, organizations deal withmul-
in related work [57], [64], [96] but they have not been identi- tiple objectives and look for optimal trade-off solutions that
fied as top influential factors before. Hence, our results sug- balance several criteria. Future research should investigate
gest that different environmental aspects may play a larger how the value of on-time delivery is measured and weighed
role in on-time delivery than previously thought. Further in trade-offs in software industry.
research is required to investigate the impact of the organiza- A Sociotechnical Approach to Improving On-Time Delivery.
tional environment on on-time delivery in different settings. The perceived indirect and contributory relationships

Coordination Challenges in Large-Scale Agile. Our survey between influential factors (see Table 2) show that changing
respondents indicated that several factors affecting on-time one factor may impact another, and that there are many
delivery are related to the challenges of adopting agile non-technical factors that may also have an impact on on-
methods at the large scale of the case company. These fac- time delivery. The results lead to some hypotheses that we
tors include task dependencies, technical dependencies, discussed in Section 5, that may suggest corrective actions
geographic distribution and organizational alignment. Our to improve the timeliness of software deliveries. The hierar-
respondents explained that their teams often depend on chical interactions between factors indicate that the inter-
other teams and external parties for testing and deploying play between technical factors and on-time delivery is
new software. This resonates with the findings by earlier influenced by the social context of development work, as
work [93], [97], [98] that large-scale agile projects are more determined by organizational and people factors. This sug-
likely to be hampered by communication and coordination gests that a healthy team culture and organizational envi-
challenges. Further research is required to investigate the ronment can be used as intervening factors to resolve
characteristics and impacts of different types of dependen- potential harmful effects of technical issues on on-time
cies and task relationships in large-scale settings. Software delivery.
organizations would benefit from mechanisms that make Predicting Delays in Software Deliveries. An interesting
teams aware of inter-team dependencies, blockers and opportunity for future work is to incorporate the influential
external dependencies that have the largest impact on their factors from our study into predictive models for delays in
delivery time. Future research should study how existing software deliveries. Currently, most software organizations
coordination mechanisms are used in large-scale agile com- rely on experts’ subjective assessment to arrive at a time
panies and how they can be improved to better support estimate for their deliveries. This may lead to inaccuracy
agile teams in delivering on time. and more importantly inconsistencies between estimates.

Incident Management Workflows. In line with earlier work Therefore, software organizations can benefit from predic-
[60], [99], [100], [101], [102], we found that software deliveries tive models that provide automated support for project
are delayed by unexpected bugs and technical incidents. In managers in predicting the delivery time or delay of soft-
our regression model, the number of unplanned stories and ware deliveries. Existing models [41], [67], [67], [107] learn
the number of incidents have a strong relationship with based on metadata (e.g., type, priority) and/or textual fea-
schedule deviation in epics. The disruptive nature of bugs tures of the software task. Our results show that the predic-
and incidents calls for streamlined incident management pro- tive power of these models can be enhanced by
cesses and automated incident handling. Promising research incorporating the significant variables from our regression
in this direction has been carried out by Gupta et al. [103]. analysis. This will enable models to capture information
They used information integration techniques and machine about the software task as well as the environment in which
learning to automatically link incoming incidentswith config- the delivery takes place.
uration items. An interesting extension would be to leverage Our findings also suggest that an incremental learning
probabilistic modeling to predict the impact of an incoming approach might be beneficial for predicting delays in soft-
incident on the time estimate of a delivery. ware deliveries. As unexpected events related to bugs, inci-

Multi-Objective Optimization for Software Delivery. We dents, and a team members’ departure can occur during the
found that the security level of a software delivery is posi- development phase (after a time estimate has been made),
tively related to delay. Our respondents indicated that there the prediction model should learn over time and adjust pre-
is no tolerance for failure in some of the business-critical sys- dictions based on newly acquired knowledge. A sliding win-
tems at ING. In highly regulated projects, engineers may dow-setting could be used to model team dynamics and
need to decide to delay a delivery to increase time available external changes. This boosts the ability of the model to learn
for quality assurance and security testing. This alludes to a and predict based on a team’s recent delivery performance,
tension between delivery speed and the constraints imposed and to forget older, irrelevant data. Initial work in this direc-
by regulations. New methods for rapid security verification tion has been carried out byAbrahamsson et al. [34].
and vulnerability identification could help organizations A Relational Theory of On-Time Delivery. A relational the-
maintain agility. Related work [104], [105], [106] has focused ory of on-time delivery would provide insightful and
on integrating security into agile methods and the challenges actionable information for project managers to design effec-
which this presents. Fitzgerald et al. [104] looked into the tive risk management strategies that improve on-time deliv-
concept of continuous compliance and end-to-end traceability ery performance. Our framework contributes in the
to support agile development processes in large-scale regu- advancement of a relational theory of on-time software deliv-
lated environments. ery [108]. We identified and categorized influential factors

The trade-off between timely delivery and security high- (descriptive theory), and aim to specify the relations between
lights a broader theme that predictable delivery is not the factors to start to explain how they interact with each other.
only factor that development teams in software organizations One potential research direction is therefore to investigate



3588 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

why factors are related, e.g., through causality. This could Although we control for variations using a large number
be assessed by causal inference on time-series data, or of participants and projects, we cannot generalize our con-
through observations collected by experiments with devel- clusions to other organizations. Replication of this work in
opment teams. different development settings is required to determine

Guiding Future Research. We anticipate that our concep- how work context influences (the perceptions of) factors
tual framework, and the set of factors we identified, can be affecting on-time delivery. Our findings indicate a trade-off
useful for other researchers that study on-time software between timely delivery and increased (security) testing
delivery. Our survey instrument and mixed-methods and code refactoring effort. In a financial organization like
approach can be replicated to reveal how specific factors ING there is no tolerance for failure in some of their busi-
impact on-time delivery in other settings. A consideration ness-critical systems. This may have influenced the factors
across organizations of different scale and context could we identified, making our findings likely to generalize more
lead to quite different factors that influence the results. to software organizations with similar security regulations.
Moreover, other social contextual settings, such as organiza- Moreover, our results show several factors related to organi-
tions with different levels of management support and orga- zational fragmentation (e.g., dependencies, organizational
nizational stability, could be considered to explore how the alignment) which may be more common in large-scale
social setting affects the interplay between technical and organizations. In an effort to increase external validity and
social factors, and their impact on on-time epic delivery. encourage replication, we have made our survey instrument

available so that others can deploy it in different organiza-
tions and contexts [72].

7 THREATS TO VALIDITY Internal Validity.We recognize that surveys can introduce
biases and may contain ambiguous questions [109]. To miti-

In this section, we discuss the threats to validity of our gate these issues, we used terminology familiar to the target
study and limitations with our conceptual framework. We population and piloted the survey. We updated the survey
used the checklist of Molleri et al. [109] to assess our surveys instruments based on the validation results. In addition, we
and identify threats to validity. The resulting scores from sought support from the second (confirmatory) survey and
our assessment using the checklist can be found in the sup- performed data triangulation. In the second survey (also
plemental material [72]. piloted), no new factors emerged from the open-ended

External Validity. As with any single-case empirical responses we solicited. In our survey design, we phrased
study, external threats are concerned with our ability to gen- and ordered the questions sequentially to avoid leading
eralize our results [110]. The company we studied employs questions. We also randomized the order of factors to
thousands of software engineers and has significant variety address order effects.
in terms of the products developed, the size and application Our surveywas not anonymous and therefore might have
domain (banking applications, cloud software, software been subject to social desirability bias (i.e., a respondent’s
tools). We performed random sampling and captured a possible tendency to appear in a positive light) [113]. Tomiti-
range of roles and experiences, which may improve gener- gate this risk, we let participants know that the responses
alizability [109]. Even though our sample of survey partici- would be kept confidential and evaluated in aggregated
pants is diverse, it is unlikely to be representative of form.
software managers and engineers in general. A factor that might have influenced our qualitative anal-

We conducted self-administered surveys, which may ysis is the bias induced by the involvement of the authors
suffer from non-response bias [111], [112]. Our surveys with the studied organization [109]. To counter the biases
were advertised as an “On-time Software Delivery Survey” which might have been introduced by the first two authors,
and therefore could have led to over-representation of the last author (from Delft University of Technology) helped
teams that deliver on-time. Developers from teams that are in designing survey questions and the manual coding of
often delayed might have been less comfortable about par- survey responses. In addition, we formally checked for reli-
ticipating in the surveys. Moreover, some of the factors pre- ability by computing inter-coder reliability. Another risk of
sented in the second survey were geared at technical the coding process is the loss of accuracy of the original
aspects of software development. This might have caused response due to an increased level of categorization. To mit-
participants in non-technical roles to drop out, resulting in a igate this risk, we allowed multiple codes to be assigned to
bias toward software engineers. A limitation from our sur- the same answer.
vey tool is that it does not record partially completed sur- Construct Validity. The goal of our first survey was to elicit
veys. Hence, we do not know the dropout rate and cannot influential factors and their interactions as experienced by
determine drop-out questions. This introduces a possible engineers themselves. As the factors and types of relation-
threat to external validity [109]. Our survey may have also ships come from open-ended responses, and we rigorously
been subject to self-selection bias, e.g., participants with assessed the expressedmechanisms throughmanual coding,
strong opinions about delivery deadlines might have been we argue that they accurately represent the respondents’
more likely to participate in the survey. To mitigate non- views. However, it should be noted that we did not verify
response and self-selection bias, we sent personal invita- whether participants can distinguish between affects. In the
tions, kept the survey as short as possible and were trans- second survey we asked participants about the perceived
parent about the survey length. We also sent reminders to impact of factors. To keep the survey short, each factor was
non-responders to increase the response rate and reduce the measured by a single response item. Therefore, we could not
possibility of bias. test the reliability of participants’ responses.



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3589

In our analysis of repository data, we consider data vari- 1) Requirements refinement, task dependencies, orga-
ables as constructs to meaningfully measure perceived nizational alignment, organizational politics and
influential factors. This introduces possible threats to con- the geographic distribution of teams are perceived
struct validity due to measurement errors [114]. The proxy to have the greatest impact on timely software
variables we measured may not capture the respondents’ delivery.
intended meaning of the concepts or constructs. Many fac- 2) Project size, number of dependencies, historic deliv-
tors, such as team commitment and organizational align- ery performance, team familiarity and developer
ment, are quantifiable in principle but not directly experience are the most important variables that
measurable. For example, we measured the historical on- explain schedule deviations in software deliveries.
time delivery performance of a team as a reflective indicator 3) Factors are found to interact hierarchically: organiza-
of their commitment to on-time delivery. However, the tional factors are perceived to interact with people
commitment level of team members might not be reflected factors, which in turn impact the technical factors.
in their past on-time delivery performance. A more com- Technical factors are perceived to have a direct
mon and direct way of measuring commitment is through impact on timely software delivery.
psychological attachment instruments but it was not possi- Our conceptual framework suggests multiple paths for
ble to collect such data ex post facto. action that may improve the timeliness of software deliver-

For the mapping of proxy measures to the perceived ies. Based on our findings, we identified challenging areas
influential factors we had to find acceptable trade-offs calling for further attention, related to the scalability of agile
between the preciseness of proxy measures and the avail- methods, inter-team dependencies, security concerns, the
ability of data at ING. We acknowledge that for some per- role of organizational culture, team stability and incident
ceived factors more precise alternatives can be found in management. Progress in these areas is crucial in order to
related work. However, the repository data available did become more predictable at delivering software in agile
not cover equally precise data on all factors. Furthermore, it settings.
is possible that the data variables do not accurately repre-
sent reality. For example, we calculated time-related infor- ACKNOWLEDGMENTS
mation on epics based on their planned and actual delivery

This work was partially supported by ING through AI for
dates in backlog management data. However, it might hap-

Fintech Research (AFR), an ICAI lab. We thank all the sur-
pen that teams close their deliveries too early or too late. We

vey participants at ING who provided valuable inputs for
cannot account for the impact of poor record keeping on

this study. We would also like to thank our anonymous
our results.

reviewers for their constructive feedback and suggestions
Transferability and Credibility of Our Framework. We

that greatly improved our paper.
believe that the influential factors in the conceptual frame-
work are likely transferable, to some extent, to other settings
as they relate to previous research in effort estimation and REFERENCES

project risk management. However, the specific results [1] T. Halkjelsvik and M. Jørgensen, “From origami to software

regarding the ordering of factors, factor relationships and development: A review of studies on judgment-based predic-
tions of performance time,” Psychol. Bull., vol. 138, no. 2, 2012,

regression analysis are bounded to the scale and context of Art. no. 238.
ING. How factors impact on-time software delivery may [2] M. Jørgensen, “What we do and don’t know about software
vary according to scale and context factors of development development effort estimation,” IEEE Softw., vol. 31, no. 2, pp.

37–40, Mar./Apr. 2014.
work. It is noteworthy that we were not able to validate all

[3] T. Chow and D.-B. Cao, “A survey study of critical success fac-
factors as the repository data available did not cover all per- tors in agile software projects,” J. Syst. Softw., vol. 81, no. 6, pp.
ceived influential factors. Our regression analysis does not 961–971, 2008.

exclude the importance of the non-included variables. [4] F. J. Heemstra, “Software cost estimation,” Inf. Softw. Technol.,
vol. 34, no. 10, pp. 627–639, 1992.

Additional variables would probably have been included in [5] M. Jørgensen, “A review of studies on expert estimation of soft-
our regression model if we had more data. We were not ware development effort,” J. Syst. Softw., vol. 70, no. 1/2, pp. 37–
able to triangulate the relationships for RQ1.3. Replication 60, 2004.

of this work is required to validate the findings and reach [6] B. W. Boehm and P. N. Papaccio, “Understanding and control-
ling software costs,” IEEE Trans. Softw. Eng., vol. 14, no. 10,

more general conclusions. This might help enrich the con- pp. 1462–1477, Oct. 1988.
ceptual framework. [7] A. Trendowicz, J. Mu€nch, and R. Jeffery, “State of the practice in

software effort estimation: A survey and literature review,”
in Proc. IFIP Central East Eur. Conf. Softw. Eng. Techn., 2008,
pp. 232–245.

8 CONCLUSION [8] A. Trendowicz and J. Mu€nch, “Factors influencing software
development productivity–state-of-the-art and industrial experi-

Improving the timeliness of software deliveries is a chal- ences,” Advances Comput., vol. 77, pp. 185–241, 2009.

lenge that is faced by many software organizations. In this [9] M. Usman, E. Mendes, F. Weidt, and R. Britto, “Effort estimation
in agile software development: A systematic literature review,”

paper, we identified and investigated the most relevant fac- in Proc. 10th Int. Conf. Predictive Models Softw. Eng., 2014,
tors affecting delay in large-scale agile software develop- pp. 82–91.
ment. We composed our findings in the form of a [10] M. Cohn, Agile Estimating and Planning. London, U.K.: Pearson

conceptual framework (Fig. 4) representing these factors Education, 2005.
[11] D. Leffingwell, Scaling Software Agility: Best Practices for Large

and their interactions. The key findings of this study are: Enterprises. London, U.K.: Pearson Education, 2007.



3590 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

[12] M. Usman, E. Mendes, and J. Bo€rstler, “Effort estimation in [35] P. Hearty, N. Fenton, D. Marquez, and M. Neil, “Predicting proj-
agile software development: A survey on the state of the ect velocity in XP using a learning dynamic Bayesian network
practice,” in Proc. 19th Int. Conf. Eval. Assessment Softw. Eng., model,” IEEE Trans. Softw. Eng., vol. 35, no. 1, pp. 124–137, Jan./
2015, pp. 1–10. Feb. 2009.

[13] E. Dantas, M. Perkusich, E. Dilorenzo, D. F. Santos, H. Almeida, [36] M. Choetkiertikul, H. K. Dam, T. Tran, T. T. M. Pham, A. Ghose,
and A. Perkusich, “Effort estimation in agile software develop- and T. Menzies, “A deep learning model for estimating story
ment: An updated review,” Int. J. Softw. Eng. Knowl. Eng., vol. 28, points,” IEEE Trans. Softw. Eng., vol. 45, no. 7, pp. 637–656, Jul.
no. 11n12, pp. 1811–1831, 2018. 2019.

[14] M. Cohn,User Stories Applied: For Agile Software Development. Bos- [37] J. Wen, S. Li, Z. Lin, Y. Hu, and C. Huang, “Systematic literature
ton, MA, USA: Addison-Wesley Professional, 2004. review of machine learning based software development effort

[15] K. Schwaber and M. Beedle, Agile Software Development With estimation models,” Inf. Softw. Technol., vol. 54, no. 1, pp. 41–59,
Scrum, vol. 1. Upper Saddle River, NJ, USA: Prentice Hall, 2002. 2012.

[16] K. Conboy and N. Carroll, “Implementing large-scale agile [38] P. Sharma and J. Singh, “Systematic literature review on software
frameworks: Challenges and recommendations,” IEEE Softw., effort estimation using machine learning approaches,” in Proc.
vol. 36, no. 2, pp. 44–50, Mar./Apr. 2019. Int. Conf. Next Gener. Comput. Inf. Syst., 2017, pp. 43–47.

[17] M. Paasivaara, “Adopting safe to scale agile in a globally distrib- [39] H. Zhang, L. Gong, and S. Versteeg, “Predicting bug-fixing time:
uted organization,” in Proc. IEEE 12th Int. Conf. Global Softw. An empirical study of commercial software projects,” in Proc.
Eng., 2017, pp. 36–40. 35th Int. Conf. Softw. Eng., 2013, pp. 1042–1051.

[18] M. Paasivaara, B. Behm, C. Lassenius, and M. Hallikainen, [40] L. D. Panjer, “Predicting eclipse bug lifetimes,” in Proc. 4th Int.
“Large-scale agile transformation at ericsson: A case study,” Workshop Mining Softw. Repositories, 2007, pp. 29–29.
Empir. Softw. Eng., vol. 23, no. 5, pp. 2550–2596, 2018. [41] P. Bhattacharya and I. Neamtiu, “Bug-fix time prediction models:

[19] H. F. Cervone, “Understanding agile project management meth- Can we do better?,” in Proc. 8th Work. Conf. Mining Softw. Reposi-
ods using scrum,”OCLC Syst. Serv.: Int. Digit. Library Perspectives, tories, 2011, pp. 207–210.
vol. 27, no. 1, pp. 18–22, 2011. [42] Q. Song, M. Shepperd, M. Cartwright, and C. Mair, “Software

[20] A. Trendowicz and R. Jeffery, Software Project Effort Estimation: defect association mining and defect correction effort pre-
Foundations and Best Practice Guidelines for Success. Berlin, Ger- diction,” IEEE Trans. Softw. Eng., vol. 32, no. 2, pp. 69–82, Feb.
many: Springer, 2014, pp. 277–293. 2006.

[21] V. Mahnic and T. Hovelja, “On using planning poker for estimat- [43] S. Assar, M. Borg, and D. Pfahl, “Using text clustering to predict
ing user stories,” J. Syst. Softw., vol. 85, no. 9, pp. 2086–2095, 2012. defect resolution time: A conceptual replication and an evalua-

[22] C. J. Torrecilla-Salinas, J. Seden~o, M. Escalona, and M. Mejıas, tion of prediction accuracy,” Empir. Softw. Eng., vol. 21, no. 4,
“Estimating, planning and managing agile web development pp. 1437–1475, 2016.
projects under a value-based perspective,” Inf. Softw. Technol., [44] C. Maddila, C. Bansal, and N. Nagappan, “Predicting pull
vol. 61, pp. 124–144, 2015. request completion time: A case study on large scale cloud serv-

[23] P. Abrahamsson, I. Fronza, R. Moser, J. Vlasenko, and W. ices,” in Proc. 27th ACM Joint Meeting Eur. Softw. Eng. Conf. Symp.
Pedrycz, “Predicting development effort from user stories,” in Found. Softw. Eng., 2019, pp. 874–882.
Proc. Int. Symp. Empir. Softw. Eng. Meas., 2011, pp. 400–403. [45] J. G. March and Z. Shapira, “Managerial perspectives on risk and

[24] D. Nguyen-Cong and D. Tran-Cao, “A review of effort estima- risk taking,”Manage. Sci., vol. 33, no. 11, pp. 1404–1418, 1987.
tion studies in agile, iterative and incremental software devel- [46] K. De Bakker, A. Boonstra, and H. Wortmann, “Does risk man-
opment,” in Proc. RIVF Int. Conf. Comput. Commun. Technol.-Res. agement contribute to it project success? A meta-analysis of
Innov. Vis. Future, 2013, pp. 27–30. empirical evidence,” Int. J. Project Manage., vol. 28, no. 5, pp. 493–

[25] S. Grapenthin, S. Poggel, M. Book, and V. Gruhn, “Facilitating 503, 2010.
task breakdown in sprint planning meeting 2 with an interaction [47] J. J. Jiang, G. Klein, and T. L. Means, “Project risk impact on soft-
room: An experience report,” in Proc. 40th EUROMICRO Conf. ware development team performance,” Project Manage. J., vol. 31,
Softw. Eng. Adv. Appl., 2014, pp. 1–8. no. 4, pp. 19–26, 2000.

[26] S. Kang, O. Choi, and J. Baik, “Model-based dynamic cost estima- [48] J. J. Jiang, G. Klein, and R. Discenza, “Information system success
tion and tracking method for agile software development,” in as impacted by risks and development strategies,” IEEE Trans.
Proc. IEEE/ACIS 9th Int. Conf. Comput. Inf. Sci., 2010, pp. 743–748. Eng. Manage., vol. 48, no. 1, pp. 46–55, Feb. 2001.

[27] I. Inayat, S. S. Salim, S. Marczak, M. Daneva, and S. Shamshir- [49] W.-M. Han and S.-J. Huang, “An empirical analysis of risk com-
band, “A systematic literature review on agile requirements ponents and performance on software projects,” J. Syst. Softw.,
engineering practices and challenges,” Comput. Hum. Behav., vol. vol. 80, no. 1, pp. 42–50, 2007.
51, pp. 915–929, 2015. [50] L. Wallace, M. Keil, and A. Rai, “Understanding software project

[28] M. Agrawal and K. Chari, “Software effort, quality, and cycle risk: A cluster analysis,” Inf. Manage., vol. 42, no. 1, pp. 115–125,
time: A study of CMM level 5 projects,” IEEE Trans. Softw. Eng., 2004.
vol. 33, no. 3, pp. 145–156, Mar. 2007. [51] B. W. Boehm, “Software risk management: Principles and

[29] M. Jørgensen and S. Grimstad, “Avoiding irrelevant and mis- practices,” IEEE Softw., vol. 8, no. 1, pp. 32–41, Jan. 1991.
leading information when estimating development effort,” IEEE [52] M. J. Carr, S. L. Konda, I. Monarch, F. C. Ulrich, and C. F. Walker,
Softw., vol. 25, no. 3, pp. 78–83, May/Jun. 2008. “Taxonomy-based risk identification,” Carnegie-Mellon Univ.,

[30] T. Menzies, Z. Chen, J. Hihn, and K. Lum, “Selecting best practi- Pittsburgh, PA, Tech. Rep. CMU/SEI-93-TR-006, 1993.
ces for effort estimation,” IEEE Trans. Softw. Eng., vol. 32, no. 11, [53] R. N. Charette, Software Engineering Risk Analysis and Manage-
pp. 883–895, Nov. 2006. ment. New York, NY, USA: Intertext Publications, 1989.

[31] M. Jørgensen and T. M. Gruschke, “The impact of lessons- [54] C. Jones, Assessment and Control of Software Risks. Englewood
learned sessions on effort estimation and uncertainty asses- Cliffs, NJ, USA: Yourdon Press, 1994.
sments,” IEEE Trans. Softw. Eng., vol. 35, no. 3, pp. 368–383, [55] T. Addison and S. Vallabh, “Controlling software project
May/Jun. 2009. risks: An empirical study of methods used by experienced proj-

[32] A. Trendowicz, M. Ochs, A. Wickenkamp, J. Mu€nch, Y. Ishigai, ect managers,” in Proc. Annu. Res. Conf. South African Inst. Com-
and T. Kawaguchi, “Integrating human judgment and data anal- put. Scientists Inf. Technol. Enablement Through Technol., 2002,
ysis to identify factors influencing software development pp. 128–140.
productivity,” e-Informatica, vol. 2, no. 1, pp. 47–69, 2008. [56] H. Barki, S. Rivard, and J. Talbot, “Toward an assessment of soft-

[33] E. Kocaguneli, T. Menzies, and J. W. Keung, “On the value of ware development risk,” J. Manage. Inf. Syst., vol. 10, no. 2, pp.
ensemble effort estimation,” IEEE Trans. Softw. Eng., vol. 38, no. 203–225, 1993.
6, pp. 1403–1416, Nov./Dec. 2012. [57] R. Schmidt, K. Lyytinen, M. Keil, and P. Cule, “Identifying soft-

[34] P. Abrahamsson, R. Moser, W. Pedrycz, A. Sillitti, and G. Succi, ware project risks: An international delphi study,” J. Manage. Inf.
“Effort prediction in iterative software development processes– Syst., vol. 17, no. 4, pp. 5–36, 2001.
incremental versus global prediction models,” in Proc. 1st Int. [58] K. Ewusi-Mensah, Software Development Failures. Cambridge,
Symp. Empir. Softw. Eng. Meas., 2007, pp. 344–353. MA, USA: MIT Press, 2003.



KULA ETAL.: FACTORS AFFECTING ON-TIME DELIVERY IN LARGE-SCALE AGILE SOFTWARE DEVELOPMENT 3591

[59] S. L. Jarvenpaa and B. Ives, “Executive involvement and partici- [84] Y. Miyazaki, M. Terakado, K. Ozaki, and H. Nozaki, “Robust
pation in the management of information technology,” MIS regression for developing software estimation models,” J. Syst.
Quart., vol. 15, pp. 205–227, 1991. Softw., vol. 27, no. 1, pp. 3–16, 1994.

[60] M. Van Genuchten, “Why is software late? An empirical study of [85] J. H. Friedman, “Multivariate adaptive regression splines,” The
reasons for delay in software development,” IEEE Trans. Softw. Ann. Statist., vol. 19, pp. 1–67, 1991.
Eng., vol. 17, no. 6, pp. 582–590, Jan. 1991. [86] J. Noll and S. Beecham, “Measuring global distance: A survey of

[61] H. Barki and J. Hartwick, “Rethinking the concept of user distance factors and interventions,” in Proc. Int. Conf. Softw. Pro-
involvement,”MIS Quart., vol. 13, pp. 53–63, 1989. cess Improvement Capability Determination, 2016, pp. 227–240.

[62] H. Hoodat and H. Rashidi, “Classification and analysis of risks in [87] D. Hartmann and R. Dymond, “Appropriate agile measurement:
software engineering,” World Acad. Sci. Eng. Technol., vol. 56, no. Using metrics and diagnostics to deliver business value,” in Proc.
32, pp. 446–452, 2009. AGILE, 2006, pp. 6–134.

[63] J. Ropponen and K. Lyytinen, “Components of software develop- [88] S. E. Dreyfus and H. L. Dreyfus, “A five-stage model of the
ment risk: How to address them? A project manager survey,” mental activities involved in directed skill acquisition,” Distribu-
IEEE Trans. Softw. Eng., vol. 26, no. 2, pp. 98–112, Feb. 2000. tion, Univ. California, Berkeley, Berkeley, CA, p. 22, Feb. 1980.

[64] L. Wallace, M. Keil, and A. Rai, “How software project risk [89] C. Aube and V. Rousseau, “Team goal commitment and team
affects project performance: An investigation of the dimensions effectiveness: The role of task interdependence and supportive
of risk and an exploratory model,” Decis. Sci., vol. 35, no. 2, pp. behaviors,” Group Dynamics: Theory Res. Pract., vol. 9, no. 3, 2005,
289–321, 2004. Art. no. 189.

[65] J. Menezes, C. Gusm~ao, and H. Moura, “Risk factors in software [90] K. M. Chudoba, E. Wynn, M. Lu, and M. B. Watson-Manheim,
development projects: A systematic literature review,” Softw. “How virtual are we? Measuring virtuality and understanding
Qual. J., vol. 27, no. 3, pp. 1149–1174, 2019. its impact in a global organization,” Inf. Syst. J., vol. 15, no. 4, pp.

[66] E. Letier, D. Stefan, and E. T. Barr, “Uncertainty, risk, and infor- 279–306, 2005.
mation value in software requirements and architecture,” in [91] O. Morgenshtern, T. Raz, and D. Dvir, “Factors affecting dura-
Proc. 36th Int. Conf. Softw. Eng., 2014, pp. 883–894. tion and effort estimation errors in software development proj-

[67] M. Choetkiertikul, H. K. Dam, T. Tran, and A. Ghose, “Predicting ects,” Inf. Softw. Technol., vol. 49, no. 8, pp. 827–837, 2007.
delays in software projects using networked classification (t),” [92] R. S. Huckman, B. R. Staats, and D. M. Upton, “Team familiarity,
in Proc. 30th IEEE/ACM Int. Conf. Autom. Softw. Eng., 2015, role experience, and performance: Evidence from indian soft-
pp. 353–364. ware services,”Manage. Sci., vol. 55, no. 1, pp. 85–100, 2009.

[68] H. R. Joseph, “Poster: Software development risk management: [93] K. Dikert, M. Paasivaara, and C. Lassenius, “Challenges and suc-
Using machine learning for generating risk prompts,” in Proc. cess factors for large-scale agile transformations: A systematic lit-
IEEE/ACM 37th Int. Conf. Softw. Eng., 2015, vol. 2, pp. 833–834. erature review,” J. Syst. Softw., vol. 119, pp. 87–108, 2016.

[69] H. Kniberg and A. Ivarsson, “Scaling agile@ spotify with tribes, [94] H. Huang and E. M. Trauth, “Cultural influences and globally
squads, chapters& guilds,” Entry Posted November, vol. 12, pp. 1–14, distributed information systems development: Experiences from
2012. chinese it professionals,” in Proc. ACM SIGMIS CPR Conf.

[70] B. A. Kitchenham and S. L. Pfleeger, “Principles of survey Comput. Personnel Res.: The Global Inf. Technol. Workforce, 2007,
research: Parts 1– 6,” ACM SIGSOFT Softw. Eng. Notes, vol. 28, pp. 36–45.
pp. 24–27, 2003. [95] M. Kajko-Mattsson, “Problems in agile trenches,” in Proc. 2nd

[71] M. Kasunic, “Designing an effective survey,” Carnegie-Mellon ACM-IEEE Int. Symp. Empir. Softw. Eng. Meas., 2008, pp. 111–119.
Univ., Pittsburgh, PA, Tech. Rep. CMU/SEI-2005-HB-004, 2005. [96] J. Kontio, M. Hoglund, J. Ryden, and P. Abrahamsson,

[72] E. Kula, E. Greuter, A. van Deursen, and G. Gousios, “Managing commitments and risks: Challenges in distributed
“Supplemental material for factors affecting on-time delivery in agile development,” in Proc. 26th Int. Conf. Softw. Eng., 2004, pp.
large-scale agile software development,” 2020. [Online]. Avail- 732–733.
able: https://tinyurl.com/yxkylqpb [97] N. Sekitoleko, F. Evbota, E. Knauss, A. Sandberg, M. Chaudron,

[73] M. Jorgensen and K. Molokken-Ostvold, “Reasons for software and H. H. Olsson, “Technical dependency challenges in large-
effort estimation error: Impact of respondent role, information scale agile software development,” in Proc. Int. Conf. Agile Softw.
collection approach, and data analysis method,” IEEE Trans. Develop., 2014, pp. 46–61.
Softw. Eng., vol. 30, no. 12, pp. 993–1007, Dec. 2004. [98] D. E. Strode, S. L. Huff, B. Hope, and S. Link, “Coordination in

[74] F. Strack, ““Order effects” in survey research: Activation and co-located agile software development projects,” J. Syst. Softw.,
information functions of preceding questions,” in Context Effects vol. 85, no. 6, pp. 1222–1238, 2012.
in Social and Psychological Research. Berlin, Germany: Springer, [99] A. Elbanna and S. Sarker, “The risks of agile software develop-
1992, pp. 23–34. ment: Learning from adopters,” IEEE Softw., vol. 33, no. 5, pp.

[75] J. S. Molleri, K. Petersen, and E. Mendes, “Survey guidelines in 72–79, Sep./Oct. 2016.
software engineering: An annotated review,” in Proc. 10th ACM/ [100] A. L. Lederer and J. Prasad, “Causes of inaccurate software
IEEE Int. Symp. Empir. Softw. Eng. Meas., 2016, pp. 1–6. development cost estimates,” J. Syst. Softw., vol. 31, no. 2, pp.

[76] J. Y. Cho and E.-H. Lee, “Reducing confusion about grounded 125–134, 1995.
theory and qualitative content analysis: Similarities and differ- [101] S. Grimstad, M. Jorgensen, and K. Molokken-Ostvold, “The cli-
ences,” Qualitative Rep., vol. 19, no. 32, pp. 1–20, 2014. ents’ impact on effort estimation accuracy in software develop-

[77] J. F. DeFranco and P. A. Laplante, “A content analysis process for ment projects,” in Proc. 11th IEEE Int. Softw. Metrics Symp., 2005,
qualitative software engineering research,” Innovations Syst. pp. 10–pp.
Softw. Eng., vol. 13, no. 2, pp. 129–141, 2017. [102] K. M. Furulund and K. Molkken-stvold, “Increasing software

[78] J. Cohen, “A coefficient of agreement for nominal scales,” Educ. effort estimation accuracy using experience data, estimation
Psychol. Meas., vol. 20, no. 1, pp. 37–46, 1960. models and checklists,” in Proc. 7th Int. Conf. Qual. Softw., 2007,

[79] P. Judea, “Causality: Models, reasoning, and inference,” Cam- pp. 342–347.
bridge University Press. ISBN 0, vol. 521, no. 77362, 2000, Art. no. 8. [103] R. Gupta, K. H. Prasad, and M. Mohania, “Automating ITSM

[80] S. Conte, H. Dunsmore, and V. Shen, Software Engineering Metrics incident management process,” in Proc. Int. Conf. Auton. Comput.,
and Models. Redwood City, CA, USA: Benjamin-Cummings Pub- 2008, pp. 141–150.
lishing Co., Inc., 1986. [104] B. Fitzgerald, K.-J. Stol, R. O’Sullivan, and D. O’Brien, “Scaling

[81] T. Foss, E. Stensrud, B. Kitchenham, and I. Myrtveit, “A simula- agile methods to regulated environments: An industry case
tion study of the model evaluation criterion MMRE,” IEEE Trans. study,” in Proc. 35th Int. Conf. Softw. Eng., 2013, pp. 863–872.
Softw. Eng., vol. 29, no. 11, pp. 985–995, Nov. 2003. [105] F. Moyon, K. Beckers, S. Klepper, P. Lachberger, and B. Bruegge,

[82] B. A. Kitchenham, L. M. Pickard, S. G. MacDonell, and M. J. “Towards continuous security compliance in agile software
Shepperd, “What accuracy statistics really measure,” IEE Proc.- development at scale,” in Proc. IEEE/ACM 4th Int. Workshop Rapid
Softw., vol. 148, no. 3, pp. 81–85, 2001. Continuous Softw. Eng., 2018, pp. 31–34.

[83] D. Port and M. Korte, “Comparative studies of the model evalua- [106] L. ben Othmane, P. Angin, H. Weffers, and B. Bhargava,
tion criterions MMRE and PRED in software cost estimation “Extending the agile development process to develop acceptably
research,” in Proc. 2nd ACM-IEEE Int. Symp. Empir. Softw. Eng. secure software,” IEEE Trans. Dependable Secure Comput., vol. 11,
Meas., 2008, pp. 51–60. no. 6, pp. 497–509, Nov./Dec. 2014.



3592 IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 48, NO. 9, SEPTEMBER 2022

[107] E. Giger, M. Pinzger, and H. Gall, “Predicting the fix time of Eric Greuter is chief product owner at the IT
bugs,” in Proc. 2nd Int. Workshop Recommendation Syst. Softw. Infrastructure Data & Analytics Department of
Eng., 2010, pp. 52–56. ING TECH, Netherlands. He leads several engi-

[108] F. S. Downs and J. Fawcett, The Relationship of Theory and Research. neering and research teams, and is responsible
London, U.K.: McGraw-Hill/Appleton & Lange, 1986. for the backlog management data warehouse at

[109] J. S. Molleri, K. Petersen, and E. Mendes, “An empirically evalu- ING, Netherlands. His research interests include
ated checklist for surveys in software engineering,” Inf. Softw. software processes, data integration, and cloud
Technol., vol. 119, 2020, Art. no. 106240. infrastructure services.

[110] R. Donmoyer, “Generalizability and the single-case study,” in
Case Study Method: Key Issues, Key Texts. Thousand Oaks, CA,
USA: SAGE Publications, 2000, pp. 45–68.

[111] E. D. De Leeuw, Data Quality in Mail, Telephone and Face to Face
Surveys. San Jose, CA, USA: ERIC, 1992.

[112] R. M. Groves, R. B. Cialdini, and M. P. Couper, “Understanding Arie van Deursen (Member, IEEE) is a full pro-
the decision to participate in a survey,” Public Opin. Quart., vol. fessor in software engineering at the Delft Univer-
56, no. 4, pp. 475–495, 1992. sity of Technology, The Netherlands, where he

[113] A. Furnham, “Response bias, social desirability and dissim- heads the Software Engineering Research Group
ulation,” Pers. Indiv. Differ., vol. 7, no. 3, pp. 385–400, 1986. and chairs the Department of Software Technol-

[114] P. Ralph and E. Tempero, “Construct validity in software engi- ogy. His research interests include human
neering research and software metrics,” in Proc. 22nd Int. Conf. aspects of software engineering, software archi-
Eval. Assessment Softw. Eng., 2018, pp. 13–23. tecture, and software testing. He serves on the

advisory board of the Innovation Center for AI

Elvan Kula (Member, IEEE) is currently working (ICAI). He is founder and scientific director of AI

toward the doctoral degree at the Delft University for Fintech Research (AFR), and one of the cur-

of Technology, The Netherlands. She focuses on rently 20 ICAI labs. He served as program co-chair for ESEC/FSE 2017

using automated techniques to both understand and for ICSE 2021. He serves on the advisory board of the Empirical

and improve software development processes in Software Engineering and on the editorial board of the PeerJ Computer

terms of efficiency and predictability. Her Science.

research interests include effort estimation, soft-
ware analytics, and machine learning for software
engineering. She is the manager of AI for Fintech Georgios Gousios is a research engineer at
Research, a five year research collaboration Facebook and an associate professor at the Delft
between ING and TU Delft. University of Technology, The Netherlands (on

leave). He works in the fields of software analyt-
ics, software ecosystems, software processes,
and machine learning for software engineering.
He is the main author of the GHTorrent data col-
lection and curation framework and various
widely used tools and datasets.

" For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/csdl.